# default config
project_name: 'efficient_training'
work_root: 'efficient_diffusion_training'
exp_name: 'exp'
llama_path: 'meta-llama/Llama-3.2-1B'

model:
  flashSA: 'Joint_SA' # choose from ['none', 'Joint_SA']
  patch_size: 1
  latent_size: 16
  in_channels: 32
  out_channels: 32
  caption_max_seq_length: 120
  caption_channels: 2048
  #arch: 'sub'

  # args for REPA
  repa_depth: -1 # [0, num_blocks-1], None for not using
  projector_dim: 2048 
  z_dims: [768]

training:
  gradient_accumulation_steps: 1
  train_batch_size: 256
  num_workers: 16
  num_warmup_steps: 500
  num_train_timesteps: 1000
  fm_shift: 1.0 # flow matching shift
  max_iters: 200000
  class_dropout_prob: 0.1 # dropping prob for classifier-free guidance
  null_emb_path: 'data/null_emb.pth'
  scaling_factor: 0.41407 # scaling factor for VAE
  save_freq: 5000
  grad_checkpointing: False
  nblocks_to_apply_grad_checkpointing: -1 # -1 for all blocks
  gradient_clip: 1.0
  resume_from: ''
  start_step: 0
  use_ema: False
  ema_rate: 0.9999
  proj_coeff: 0.5

  # args for timestep sampling
  weighting_scheme: 'logit_normal' 
  logit_mean: 0
  logit_std: 1
  mode_scale: 1.29

  # args for deferred masking
  patch_mixer_depth: -1 # [0, num_blocks-1], None for not using
  mask_ratio: 0.5 # ratio for masking

  transformer_ckpt: ''

validation:
  validation_frequency: 1000
  seed: 0
  num_inference_timesteps: 20
  cfg: 4.5
  num_images_per_prompt: 1

optimizer:
  type: 'AdamW'
  opt_kwargs: 
    lr: 1e-4
    weight_decay: 3e-2
    eps: 1e-10



dataset:
  use_dummy_data: False
  precompute_txt_emb: False
  precompute_dino_feat: False
  datasets: [
    'train',
    'valid' 
    ]
